{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2f37b07a-7e6e-4874-8b8e-933b15af9f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import joblib\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from tensorflow.keras.models import load_model\n",
    "import librosa\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "import os\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a86893f7-37e0-4964-81af-aa36590a94a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_2196\\3921577965.py:41: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(\"best_ecg_cnn.pth\", map_location=\"cpu\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 128])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# **Updated CNN Model**\n",
    "class ECG_CNN(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(ECG_CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, 3, padding=1)  \n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(64, 128, 3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.global_avg_pool = nn.AdaptiveAvgPool2d(1)  \n",
    "        self.fc1 = nn.Linear(128, 64)  \n",
    "        self.fc2 = nn.Linear(64, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = self.pool(F.relu(self.conv3(x)))\n",
    "        x = self.global_avg_pool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "def predict_ecg_image(image_path):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # ✅ Ensure model architecture matches the saved one\n",
    "    num_classes = 4\n",
    "    model = ECG_CNN(num_classes=num_classes).to(device)\n",
    "\n",
    "    try:\n",
    "        state_dict = torch.load(\"best_ecg_cnn.pth\", map_location=device)\n",
    "        model.load_state_dict(state_dict)\n",
    "    except RuntimeError as e:\n",
    "        print(f\"Error loading ECG model: {e}\")\n",
    "        return None\n",
    "\n",
    "    model.eval()\n",
    "    return model  # Ensure the model is returned properly\n",
    "\n",
    "state_dict = torch.load(\"best_ecg_cnn.pth\", map_location=\"cpu\")\n",
    "print(state_dict[\"fc1.weight\"].shape)  # Check the expected shape\n",
    "\n",
    "def extract_ecg_features(image_path):\n",
    "    \"\"\" Extracts ECG features from an image using a trained CNN model \"\"\"\n",
    "    \n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    # Load the trained ECG model\n",
    "    model = ECG_CNN(num_classes=4).to(device)  # Ensure num_classes matches training\n",
    "    model.load_state_dict(torch.load(\"best_ecg_cnn.pth\", map_location=device))\n",
    "    model.eval()\n",
    "    \n",
    "   \n",
    "\n",
    "    # Load and preprocess the image\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Grayscale(),   # Convert to grayscale\n",
    "        transforms.Resize((128, 128)),  # Resize to match training size\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "\n",
    "    img = Image.open(image_path).convert(\"L\")\n",
    "    img = transform(img).unsqueeze(0).to(device)  # Add batch dimension\n",
    "\n",
    "    # Get features from the model\n",
    "    with torch.no_grad():\n",
    "        features = model(img)\n",
    "\n",
    "    return features.cpu().numpy().flatten()  # Convert tensor to numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4f67f2e9-0bca-417a-b862-b73a89291d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ✅ Load PPG Model\n",
    "def extract_ppg_features(csv_path):\n",
    "    model = joblib.load(\"best_ppg_model.pkl\")\n",
    "    data = pd.read_csv(csv_path).drop(columns=['Label'])\n",
    "    scaler = StandardScaler()\n",
    "    scaled_features = scaler.fit_transform(data)\n",
    "    return np.mean(scaled_features, axis=0)  # Aggregate features\n",
    "\n",
    "\n",
    "def extract_pcg_features(audio_path):\n",
    "    if not os.path.exists(\"SOUND_LSTM_model.h5\"):\n",
    "        raise FileNotFoundError(\"Trained PCG model file 'SOUND_LSTM_model.h5' not found!\")\n",
    "\n",
    "    # ✅ Ensure model input shape matches the one used during training\n",
    "    model = load_model(\"SOUND_LSTM_model.h5\", compile=False)\n",
    "\n",
    "    # Load and preprocess PCG (Phonocardiogram) audio\n",
    "    y, sr = librosa.load(audio_path, sr=None)\n",
    "    mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13)\n",
    "    mfccs_scaled = np.mean(mfccs.T, axis=0).reshape(1, -1)\n",
    "\n",
    "    return mfccs_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1929dc10-7f4f-445f-b4ae-b650c49300f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter ECG image path:  ecg/test/ECG Images of Myocardial Infarction Patients (240x12=2880)/MI(1).jpg\n",
      "Enter PPG CSV path:  PPG_Dataset.csv\n",
      "Enter PCG audio path:  heart_sound/val/unhealthy/a0001.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_2196\\3921577965.py:51: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"best_ecg_cnn.pth\", map_location=device))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ ECG Model Loaded Successfully!\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Unrecognized keyword arguments: ['batch_shape']",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[35], line 47\u001b[0m\n\u001b[0;32m     44\u001b[0m pcg_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEnter PCG audio path: \u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mall\u001b[39m([os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(ecg_path), os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(ppg_path), os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(pcg_path)]):\n\u001b[1;32m---> 47\u001b[0m     final_result \u001b[38;5;241m=\u001b[39m \u001b[43mpredict_fusion\u001b[49m\u001b[43m(\u001b[49m\u001b[43mecg_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mppg_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpcg_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     48\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     49\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError: One or more input file paths do not exist!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[35], line 17\u001b[0m, in \u001b[0;36mpredict_fusion\u001b[1;34m(ecg_path, ppg_path, pcg_path)\u001b[0m\n\u001b[0;32m     15\u001b[0m ecg_features \u001b[38;5;241m=\u001b[39m extract_ecg_features(ecg_path)\n\u001b[0;32m     16\u001b[0m ppg_features \u001b[38;5;241m=\u001b[39m extract_ppg_features(ppg_path)\n\u001b[1;32m---> 17\u001b[0m pcg_features \u001b[38;5;241m=\u001b[39m \u001b[43mextract_pcg_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpcg_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# Concatenate features\u001b[39;00m\n\u001b[0;32m     20\u001b[0m fusion_input \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mhstack((ecg_features, ppg_features, pcg_features))\n",
      "Cell \u001b[1;32mIn[34], line 15\u001b[0m, in \u001b[0;36mextract_pcg_features\u001b[1;34m(audio_path)\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrained PCG model file \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSOUND_LSTM_model.h5\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m not found!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# ✅ Ensure model input shape matches the one used during training\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mSOUND_LSTM_model.h5\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mcompile\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# Load and preprocess PCG (Phonocardiogram) audio\u001b[39;00m\n\u001b[0;32m     18\u001b[0m y, sr \u001b[38;5;241m=\u001b[39m librosa\u001b[38;5;241m.\u001b[39mload(audio_path, sr\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\engine\\input_layer.py:152\u001b[0m, in \u001b[0;36mInputLayer.__init__\u001b[1;34m(self, input_shape, batch_size, dtype, input_tensor, sparse, name, ragged, type_spec, **kwargs)\u001b[0m\n\u001b[0;32m    150\u001b[0m         input_shape \u001b[38;5;241m=\u001b[39m batch_input_shape[\u001b[38;5;241m1\u001b[39m:]\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwargs:\n\u001b[1;32m--> 152\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    153\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized keyword arguments: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(kwargs\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    154\u001b[0m     )\n\u001b[0;32m    156\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sparse \u001b[38;5;129;01mand\u001b[39;00m ragged:\n\u001b[0;32m    157\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    158\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot set both sparse and ragged to True in a Keras input.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    159\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: Unrecognized keyword arguments: ['batch_shape']"
     ]
    }
   ],
   "source": [
    "\n",
    "# ✅ Final Fusion Model\n",
    "class FusionNN(nn.Module):\n",
    "    def __init__(self, input_size, num_classes=3):  # 3 Classes: CAD, Arrhythmia, Normal\n",
    "        super(FusionNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 64)\n",
    "        self.fc2 = nn.Linear(64, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "def predict_fusion(ecg_path, ppg_path, pcg_path):\n",
    "    # Extract features\n",
    "    ecg_features = extract_ecg_features(ecg_path)\n",
    "    ppg_features = extract_ppg_features(ppg_path)\n",
    "    pcg_features = extract_pcg_features(pcg_path)\n",
    "\n",
    "    # Concatenate features\n",
    "    fusion_input = np.hstack((ecg_features, ppg_features, pcg_features))\n",
    "    \n",
    "    # Load Fusion Model\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = FusionNN(input_size=fusion_input.shape[0], num_classes=3).to(device)\n",
    "    model.load_state_dict(torch.load(\"fusion_model.pth\", map_location=device))\n",
    "    model.eval()\n",
    "\n",
    "    # Predict\n",
    "    fusion_tensor = torch.tensor(fusion_input, dtype=torch.float32).unsqueeze(0).to(device)\n",
    "    with torch.no_grad():\n",
    "        output = model(fusion_tensor)\n",
    "        _, predicted_class = torch.max(output, 1)\n",
    "\n",
    "    class_labels = {0: \"Coronary Artery Disease (CAD)\", 1: \"Arrhythmia\", 2: \"Normal\"}\n",
    "    result = class_labels.get(predicted_class.item(), \"Unknown\")\n",
    "\n",
    "    print(f\"Final Diagnosis: {result}\")\n",
    "    return result\n",
    "\n",
    "# ✅ Main Execution\n",
    "if __name__ == \"__main__\":\n",
    "    ecg_path = input(\"Enter ECG image path: \").strip()\n",
    "    ppg_path = input(\"Enter PPG CSV path: \").strip()\n",
    "    pcg_path = input(\"Enter PCG audio path: \").strip()\n",
    "\n",
    "    if all([os.path.exists(ecg_path), os.path.exists(ppg_path), os.path.exists(pcg_path)]):\n",
    "        final_result = predict_fusion(ecg_path, ppg_path, pcg_path)\n",
    "    else:\n",
    "        print(\"Error: One or more input file paths do not exist!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f18b2de-4370-4afb-a131-e72b0e4e0964",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
